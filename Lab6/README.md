# ğŸ§  Lab 6 â€“ Pix2Pix GAN (Image-to-Image Translation)

> ğŸ“Œ Course: Introduction to Generative AI  
> ğŸ‘¨â€ğŸ’» Student: Pratik Sinha  
> ğŸ§ª Experiment: Pix2Pix using U-Net + PatchGAN  

---

## ğŸš€ Overview

This lab implements **Pix2Pix**, a Conditional GAN (cGAN) for paired image-to-image translation.

The model learns to convert **edge images â†’ realistic shoe images** and compares its performance with a simple **Baseline CNN encoderâ€“decoder** model.

---

## ğŸ¯ Objectives

âœ” Implement U-Net Generator  
âœ” Implement PatchGAN Discriminator  
âœ” Train using Adversarial + L1 Loss  
âœ” Compare Pix2Pix with Baseline CNN  

---

## ğŸ“‚ Dataset

A simulated paired dataset was created using:

- ğŸ‘Ÿ FashionMNIST (shoe-related classes)
- ğŸ–Š OpenCV Canny Edge Detection

Input: Edge image  
Target: Original grayscale shoe image  

---

## ğŸ— Architecture

### ğŸ”¹ Generator â€“ U-Net
- Encoderâ€“decoder structure
- Skip connections preserve spatial information
- Final activation: Tanh

### ğŸ”¹ Discriminator â€“ PatchGAN
- Fully convolutional
- No fully connected layers
- Produces patch-level authenticity map

### ğŸ”¹ Baseline Model
- Simple CNN encoderâ€“decoder
- Trained only with L1 loss
- No adversarial training

---

## ğŸ“‰ Loss Functions

### Generator Loss

Generator Loss = GAN Loss + (100 Ã— L1 Loss)

- Adversarial Loss: BCEWithLogitsLoss
- L1 Reconstruction Loss

### Discriminator Loss

Binary classification between:
- Real pairs (edge + real)
- Fake pairs (edge + generated)

---

## âš™ Training Details

| Parameter | Value |
|-----------|--------|
| Image Size | 64Ã—64 |
| Batch Size | 64 |
| Epochs | 20 |
| Learning Rate | 0.0002 |
| Optimizer | Adam (Î²1=0.5, Î²2=0.999) |
| Device | CUDA (GPU) |

---

## ğŸ“Š Training Results

Final Loss Values After 20 Epochs:

- ğŸ”µ Discriminator Loss â‰ˆ 0.13  
- ğŸŸ¢ Generator Loss â‰ˆ 8.20  
- ğŸŸ¡ Baseline L1 Loss â‰ˆ 0.137  

Observations:
- Generator loss decreased steadily during training.
- Discriminator remained stable without collapse.
- Baseline minimized pixel loss but produced blurrier outputs.

---

## ğŸ” Observations

- Pix2Pix produces sharper and smoother outputs.
- Baseline model generates blurry and blocky images.
- Adversarial training improves perceptual realism.
- PatchGAN enhances texture-level detail.

---

## ğŸ§  Key Learnings

âœ… Understanding Conditional GANs  
âœ… Importance of adversarial learning  
âœ… Role of PatchGAN discriminator  
âœ… Difference between L1-only vs GAN training  
âœ… Why baseline CNN outputs are blurry  

---

## ğŸ Conclusion

Pix2Pix successfully learns the mapping from edge images to realistic shoe images.

Compared to the baseline CNN, Pix2Pix produces more visually convincing results due to:

- Adversarial optimization
- Patch-level discrimination
- Skip connections in U-Net

This experiment demonstrates the effectiveness of GAN-based image-to-image translation.

---

## ğŸ“ Files Included

- Lab6.ipynb â€“ Colab Implementation  
- Lab6.pdf â€“ Lab Report  
- README.md â€“ Documentation  
